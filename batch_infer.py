#!/usr/bin/env python3
"""
æ‰¹é‡æ¨ç† + ç®€æ˜“è¯„æµ‹

ç”¨æ³•ï¼ˆdocker å®¹å™¨å†…ï¼‰ï¼š
    python batch_infer.py --model /workspace/models/Qwen3-14B --lora output/final --test test_cases.jsonl
"""

import json
import argparse
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel


def load_model(model_path, lora_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=BitsAndBytesConfig(load_in_8bit=True),
        device_map="auto",
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(model, lora_path)
    model.eval()
    return model, tokenizer


def run_inference(model, tokenizer, chapter_title, paragraph):
    system = (
        "ä½ æ˜¯ä¸€ä½é‡‘èç ”æŠ¥æŒ‡æ ‡æå–ä¸“å®¶ã€‚æ ¹æ®ç»™å®šçš„ç« èŠ‚æ ‡é¢˜å’Œæ®µè½å†…å®¹ï¼Œ"
        "æå–è¢«æ·±åº¦åˆ†æçš„æ ¸å¿ƒæŒ‡æ ‡ï¼Œè¾“å‡º JSONã€‚"
        "å…ˆåœ¨ analysis ä¸­åˆ†ææ®µè½ç»“æ„ï¼Œå†ç»™å‡º metrics åˆ—è¡¨ã€‚"
        "æŒ‡æ ‡æ•°é‡ä¸å›ºå®šï¼Œæ ¹æ®æ®µè½å®é™…å†…å®¹å†³å®šã€‚"
    )
    user = f"ã€ç« èŠ‚æ ‡é¢˜ã€‘{chapter_title}\nã€æ®µè½å†…å®¹ã€‘{paragraph}"
    prompt = (
        f"<|im_start|>system\n{system}<|im_end|>\n"
        f"<|im_start|>user\n{user}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=512, temperature=0.1,
            do_sample=True, pad_token_id=tokenizer.pad_token_id,
        )
    return tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)


def char_similarity(a: str, b: str) -> float:
    """å­—ç¬¦çº§ Jaccard ç›¸ä¼¼åº¦ï¼ˆbigramï¼‰"""
    if not a or not b:
        return 0.0
    a_bigrams = {a[i:i+2] for i in range(len(a) - 1)} if len(a) > 1 else {a}
    b_bigrams = {b[i:i+2] for i in range(len(b) - 1)} if len(b) > 1 else {b}
    intersection = a_bigrams & b_bigrams
    union = a_bigrams | b_bigrams
    return len(intersection) / len(union) if union else 0.0


def match_metrics(gold_name: str, pred_name: str, threshold: float = 0.5) -> bool:
    """åˆ¤æ–­ä¸¤ä¸ªæŒ‡æ ‡åæ˜¯å¦åŒ¹é…ï¼šç²¾ç¡®ã€å­ä¸²åŒ…å«ã€æˆ–å­—ç¬¦ç›¸ä¼¼åº¦"""
    if gold_name == pred_name:
        return True
    if gold_name in pred_name or pred_name in gold_name:
        return True
    if char_similarity(gold_name, pred_name) >= threshold:
        return True
    return False


def evaluate(gold_metrics, pred_metrics):
    """è¯„æµ‹ï¼šæ¯”è¾ƒæŒ‡æ ‡åç§°å’Œç±»å‹ï¼ˆç²¾ç¡® + å­ä¸² + ç›¸ä¼¼åº¦ï¼‰"""
    gold_names = [m["metric_name"] for m in gold_metrics]
    pred_names = [m["metric_name"] for m in pred_metrics]

    # ç²¾ç¡®åŒ¹é…
    exact_hits = set(g for g in gold_names if g in pred_names)

    # å®½æ¾åŒ¹é…ï¼ˆå­ä¸² + ç›¸ä¼¼åº¦ï¼‰â€”â€” è´ªå¿ƒåŒ¹é…é¿å…é‡å¤
    matched_gold = set()
    matched_pred = set()
    match_pairs = []  # (gold_idx, pred_idx)
    for gi, g in enumerate(gold_names):
        best_score = 0
        best_pi = -1
        for pi, p in enumerate(pred_names):
            if pi in matched_pred:
                continue
            if match_metrics(g, p):
                score = char_similarity(g, p)
                if g == p:
                    score = 1.0
                if score > best_score:
                    best_score = score
                    best_pi = pi
        if best_pi >= 0:
            matched_gold.add(gi)
            matched_pred.add(best_pi)
            match_pairs.append((gi, best_pi))

    # ç±»å‹å‡†ç¡®ç‡
    gold_types = {i: m["metric_type"] for i, m in enumerate(gold_metrics)}
    pred_types = {i: m["metric_type"] for i, m in enumerate(pred_metrics)}
    type_correct = sum(1 for gi, pi in match_pairs if gold_types[gi] == pred_types[pi])

    return {
        "gold_count": len(gold_names),
        "pred_count": len(pred_names),
        "exact_hits": len(exact_hits),
        "fuzzy_hits": len(matched_gold),
        "type_correct": type_correct,
        "type_total": len(match_pairs),
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", required=True)
    parser.add_argument("--lora", required=True)
    parser.add_argument("--test", required=True)
    args = parser.parse_args()

    # åŠ è½½æµ‹è¯•æ•°æ®
    cases = []
    with open(args.test, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                cases.append(json.loads(line))

    print(f"åŠ è½½ {len(cases)} æ¡æµ‹è¯•ç”¨ä¾‹")
    print(f"åŠ è½½æ¨¡å‹...")
    model, tokenizer = load_model(args.model, args.lora)

    # é€æ¡æ¨ç†
    results = []
    total_exact = total_fuzzy = total_gold = total_pred = total_type_ok = total_type_n = 0
    count_ok = count_loose = count_wrong = 0

    for i, case in enumerate(cases):
        inp = case["input"]
        gold = case["output"]
        scenario = case.get("scenario", "unknown")

        print(f"\n{'='*60}")
        print(f"[{i+1}/{len(cases)}] {scenario} | {inp['chapter_title']}")

        raw = run_inference(model, tokenizer, inp["chapter_title"], inp["paragraph"])

        # å°è¯•è§£æ JSON
        try:
            pred = json.loads(raw)
            pred_metrics = pred.get("metrics", [])
            valid_json = True
        except json.JSONDecodeError:
            pred_metrics = []
            valid_json = False
            print(f"  âš  JSON è§£æå¤±è´¥")

        gold_metrics = gold.get("metrics", [])
        ev = evaluate(gold_metrics, pred_metrics)

        total_gold += ev["gold_count"]
        total_pred += ev["pred_count"]
        total_exact += ev["exact_hits"]
        total_fuzzy += ev["fuzzy_hits"]
        total_type_ok += ev["type_correct"]
        total_type_n += ev["type_total"]

        # ä¸¥æ ¼åŒ¹é…ï¼šæ•°é‡å®Œå…¨ä¸€è‡´ + æ¨¡ç³Šå…¨è¦†ç›–
        count_match = ev["gold_count"] == ev["pred_count"]
        strict_ok = count_match and ev["fuzzy_hits"] == ev["gold_count"]
        # å®½æ¾åŒ¹é…ï¼šæ•°é‡åå·® â‰¤1 + æ¨¡ç³Šå…¨è¦†ç›– gold
        loose_ok = abs(ev["gold_count"] - ev["pred_count"]) <= 1 and ev["fuzzy_hits"] == ev["gold_count"]

        if strict_ok:
            count_ok += 1
            status = "âœ…"
        elif loose_ok:
            count_loose += 1
            status = "ğŸŸ¡"
        else:
            count_wrong += 1
            status = "âŒ"

        gold_names = [m["metric_name"] for m in gold_metrics]
        pred_names = [m["metric_name"] for m in pred_metrics]
        print(f"  {status} æœŸæœ› {ev['gold_count']} ä¸ª: {gold_names}")
        print(f"     é¢„æµ‹ {ev['pred_count']} ä¸ª: {pred_names}")
        print(f"     ç²¾ç¡®å‘½ä¸­: {ev['exact_hits']} | æ¨¡ç³Šå‘½ä¸­: {ev['fuzzy_hits']}")

        results.append({
            "index": i + 1,
            "scenario": scenario,
            "title": inp["chapter_title"],
            "valid_json": valid_json,
            "gold_count": ev["gold_count"],
            "pred_count": ev["pred_count"],
            "exact_hits": ev["exact_hits"],
            "fuzzy_hits": ev["fuzzy_hits"],
            "status": status,
            "pred_raw": raw,
        })

    # æ±‡æ€»
    print(f"\n{'='*60}")
    print(f"è¯„æµ‹ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    print(f"  æ€»ç”¨ä¾‹: {len(cases)}")
    print(f"  ä¸¥æ ¼åŒ¹é… âœ…: {count_ok}/{len(cases)} ({100*count_ok/len(cases):.0f}%)")
    print(f"  å®½æ¾åŒ¹é… ğŸŸ¡: {count_loose}/{len(cases)} ({100*count_loose/len(cases):.0f}%) (æ•°é‡Â±1, æŒ‡æ ‡å…¨è¦†ç›–)")
    print(f"  åˆè®¡é€šè¿‡: {count_ok+count_loose}/{len(cases)} ({100*(count_ok+count_loose)/len(cases):.0f}%)")
    print(f"  æ¨¡ç³Šå¬å›: {total_fuzzy}/{total_gold} ({100*total_fuzzy/total_gold:.0f}%)" if total_gold else "")
    print(f"  ç²¾ç¡®å¬å›: {total_exact}/{total_gold} ({100*total_exact/total_gold:.0f}%)" if total_gold else "")
    print(f"  ç±»å‹å‡†ç¡®: {total_type_ok}/{total_type_n} ({100*total_type_ok/total_type_n:.0f}%)" if total_type_n else "")
    print(f"  æ•°é‡åå·®: æœŸæœ›æ€»è®¡ {total_gold} ä¸ªï¼Œé¢„æµ‹æ€»è®¡ {total_pred} ä¸ª")

    # æŒ‰åœºæ™¯åˆ†ç»„ç»Ÿè®¡
    from collections import defaultdict
    by_scenario = defaultdict(lambda: {"total": 0, "strict": 0, "loose": 0, "fail": 0})
    for r in results:
        s = r["scenario"]
        by_scenario[s]["total"] += 1
        if r["status"] == "âœ…":
            by_scenario[s]["strict"] += 1
        elif r["status"] == "ğŸŸ¡":
            by_scenario[s]["loose"] += 1
        else:
            by_scenario[s]["fail"] += 1

    print(f"\næŒ‰åœºæ™¯åˆ†ç»„ (âœ…ä¸¥æ ¼ / ğŸŸ¡å®½æ¾ / âŒå¤±è´¥):")
    for s, v in sorted(by_scenario.items()):
        print(f"  {s}: âœ…{v['strict']} ğŸŸ¡{v['loose']} âŒ{v['fail']} / {v['total']}")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    out_path = Path(args.test).with_suffix(".results.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {out_path}")


if __name__ == "__main__":
    main()
